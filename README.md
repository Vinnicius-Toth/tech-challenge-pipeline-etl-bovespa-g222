# ğŸš€ tech-challenge-pipeline-etl-bovespa-g222

Projeto Tech Challenge - Fase 2 - Engenharia de Machine Learning  
Pipeline de IngestÃ£o de Dados ETL via AWS para Dados da Bovespa

---

## ğŸ“š VisÃ£o Geral

Este projeto implementa um pipeline de dados automatizado utilizando AWS (S3, Lambda, Glue, Athena) para coletar, processar e disponibilizar dados da Bovespa. O fluxo contempla desde o scraping dos dados atÃ© a disponibilizaÃ§Ã£o em tabelas particionadas para anÃ¡lise.

---

## ğŸ—‚ï¸ Estrutura do Projeto

```
tech-challenge-pipeline-etl-bovespa-g222/
â”œâ”€â”€ docs/                   # Desenhos da arquitura (.drawio e .png)
â”œâ”€â”€ infra/                  # Infraestrutura como cÃ³digo (Terraform)
â”œâ”€â”€ scrapping_on_demand/    # Scraper para coleta dos dados da Bovespa
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ glue_aggregated/    # Glue Job para agregaÃ§Ã£o dos dados
â”‚   â”œâ”€â”€ glue_details/       # Glue Job para ingestÃ£o granular dos dados
â”‚   â””â”€â”€ lambda/             # FunÃ§Ã£o Lambda para disparo de glue_details
â””â”€â”€ .github/workflows/      # CI/CD com GitHub Actions
```

---

## ğŸ”— Pipeline de Dados

1. **Scraping**  
   O script [`main.py`](scrapping_on_demand/src/main.py) realiza o scraping dos dados da Bovespa e salva o CSV no bucket S3 de ingestÃ£o.

2. **Disparo AutomÃ¡tico**  
   O upload do arquivo CSV no S3 aciona a funÃ§Ã£o Lambda [`index.py`](services/lambda/index.py), que inicia o Glue Job de ingestÃ£o detalhada.

3. **IngestÃ£o Detalhada (Glue Details)**  
   O Glue Job [`main.py`](services/glue_details/app/src/main.py) lÃª o CSV, normaliza e carrega os dados na tabela particionada do Glue Catalog.

4. **AgregaÃ§Ã£o (Glue Aggregated)**  
   Ao final da ingestÃ£o detalhada, um novo Glue Job [`main.py`](services/glue_aggregated/app/src/main.py) Ã© disparado para gerar agregaÃ§Ãµes e salvar em outra tabela.

5. **Consulta e AnÃ¡lise**  
   Os dados processados podem ser consultados via Athena, utilizando as tabelas particionadas.

---

## âš™ï¸ Arquitetura

![alt text](docs/arquitetura_pipeline_ibovespa.png)


## ğŸ” Como Executar

### 1. Scraping e Upload

Antes de executar o scraper, Ã© **obrigatÃ³rio criar um arquivo `.env`** na pasta `scrapping_on_demand/src` contendo as credenciais AWS.  
Essas credenciais sÃ£o utilizadas para autenticaÃ§Ã£o e upload dos dados para o bucket S3.

Exemplo de `.env`:

```
AWS_ACCESS_KEY_ID=SEU_ACCESS_KEY
AWS_SECRET_ACCESS_KEY=SEU_SECRET_KEY
AWS_DEFAULT_REGION=us-east-1
```

Em seguida, execute o scraper localmente para coletar e enviar os dados:

```sh
cd scrapping_on_demand/src
python main.py
```

### 2. Pipeline Automatizado

O restante do pipeline Ã© automatizado via eventos S3, Lambda e Glue.

---

## ğŸ“ VariÃ¡veis Importantes

Veja [infra/terraform/variables.tf](infra/terraform/variables.tf) para nomes de buckets, jobs e configuraÃ§Ãµes.

---

## ğŸ”„ CI/CD

O provisionamento da infraestrutura AWS (buckets S3, Glue Jobs, Lambda, etc.) Ã© realizado automaticamente via **GitHub Actions**, conforme definido no workflow [.github/workflows/terraform.yml](.github/workflows/terraform.yml).

Ao realizar um push ou pull request para a branch `main`, o pipeline executa os seguintes passos:

- Inicializa e aplica o Terraform em [infra/terraform](infra/terraform)
- Faz o empacotamento (`zip`) e upload do cÃ³digo da Lambda para o bucket de artefatos
- Faz o upload dos scripts dos Glue Jobs para o bucket de artefatos

Todo o processo Ã© automatizado, nÃ£o sendo necessÃ¡rio executar comandos Terraform manualmente. Basta garantir que as secrets AWS estejam configuradas no repositÃ³rio.

Para mais detalhes, consulte o workflow em [.github/workflows/terraform.yml](.github/workflows/terraform.yml).

---

## ğŸ§° Tecnologias Utilizadas

- Python 3.11+
- Spark
- Selenium
- BeautifulSoup4
- Pandas
- AWS Glue
- AWS Lambda
- Terraform
- GitHub Actions

---

## ğŸ‘¨â€ğŸ’» Desenvolvedores

- Vinnicius Toth - vinni.toth@gmail.com
- G222 Team â€“ FIAP Tech Challenge 2

---